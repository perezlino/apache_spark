CLASE 1
=======

Hablemos del término de big data, hay que entender a qué se refiere. Para entenderlo vamos a poner un ejemplo técnico, 
que de hecho va a ser una de las primeras cosas que implementaremos en su momento, supongamos que viene una persona de 
negocio y tiene una necesidad de negocio, esa necesidad de negocio pues esta persona te la va a expresar por medio de 
documentos, por medio de presentaciones, por medio de correos electrónicos, hablando, etc. Lo importante es que tiene 
una necesidad, esa necesidad es recibida por un desarrollador y el desarrollador lo que hace es codificar una solución 
de esa necesidad. Cuando una necesidad de negocio está colocada como código, a eso se le conoce como la LÓGICA DE NEGOCIO. 
¿Qué tipos de necesidades de negocio podría tener una persona de negocio? pues podría ser algo tan simple como un reporte: 
“ … oye quiero que me construyas un reporte que me filtre solamente a aquellos clientes que tengan más de 25 años … “ 
sí sabes de SQL, por ejemplo, eso es tan simple como lanzar una consulta WHERE donde la edad sea mayor a 25, entonces, 
podría pedirte algo muy simple o podría pedirte algo muy complejo, como por ejemplo, quiero que me construyas un modelo 
analítico que prediga si este cliente va a ser un buen pagador o va a ser un mal pagador, eso por ejemplo lo podríamos 
implementar con una red neuronal como veremos entre las últimas sesiones. Entonces, puede ser una necesidad simple o puede 
ser una necesidad muy compleja, pero es una necesidad que tiene negocio, nosotros tomamos esa necesidad, hacemos un proceso 
de análisis y tenemos la codificación. De hecho, parte del curso es también entender ese proceso de análisis y para eso 
justamente están los arquetipos que indican las capas que hay que implementar respecto a una arquitectura. Sea como sea, en 
código ya tenemos la solución a esa necesidad, cuando está en código es la lógica de negocio. Ahora vamos a ejecutar el 
proceso, para que un proceso funcione pues probablemente necesita “data input”, habrá un archivo de datos, el proceso hará 
lo que tenga que hacer y aquí está el archivo de datos de salida que es el reporte que negocio me ha pedido. En esencia esto 
es procesar, me da igual si era un reporte o una red neuronal, sigue el mismo patrón, leer, procesar y escribir la resultante. 
Pero vamos a poner más detalle técnico de esto, ¿qué implica procesar? lo primero que hay que saber es que pues negocio nos va 
a dejar uno o varios archivos de datos a procesar para resolver esa necesidad que nos haya pedido, para simplificar el concepto 
vamos a decir que nos da solo un archivo y este archivo pesa poquísimo, solamente 10 MB de datos, entonces, nosotros tenemos 
un proceso que hemos implementado, ¿cómo implementamos los procesos? para eso existen los lenguajes de programación. Para 
implementar esta lógica de negocio podríamos usar por ejemplo JAVA, R, .NET, PYTHON, SCALA, SQL, etc. Literalmente hay decenas 
y decenas de lenguajes que se utilizan dentro de las empresas, la pregunta ahora es y ¿cuál de todos estos lenguajes es el mejor 
lenguaje de programación? pues la respuesta es que no existe un mejor lenguaje de programación, sino que, cada lenguaje se adapta 
a cierta realidad. Por ejemplo, JAVA es muy bueno para la programación de frameworks empresariales, R es muy bueno para la 
programación estadística, .NET es muy bueno para ecosistemas basados en Windows, PYTHON es muy bueno para la programación 
funcional, SCALA es muy bueno para la programación funcional y para la programación de tiempo real, SQL es muy bueno para el 
procesamiento estructural y así sucesivamente. Entonces, dependiendo de lo que negocio te pida alguno de estos lenguajes de 
programación se adaptará mejor para poder codificarlo más fácilmente. Entonces tú como desarrollador dirás: “ … de acuerdo voy a 
hacer esta solución con algún lenguaje que me guste … “ por ahora voy a poner simplemente a Scala, ya en unos momentos vamos a 
aclarar el por qué debemos usar Scala para entornos de big data, de hecho, es o bien Scala o bien Python, pero vamos por partes, 
eliges un lenguaje que a ti te gusta y listo tenemos el proceso. Pero el proceso que tú has codificado se va a ejecutar en alguna 
infraestructura, no se ejecuta en el aire, de acuerdo entonces vamos a hacer más estrictos. Esta cajita de procesamiento que tomará 
la data input y va a ejecutar un script de código en donde has implementado tu lógica de negocio en el lenguaje que mejor se adapte 
tiene que tener asociada una infraestructura de procesamiento probablemente tú puedas decir: “ … oye negocio no hay ningún problema, 
mi laptop que tiene 16 GB de RAM puede procesar este pequeño archivo de 10 MB, lo va a poder hacer … “. De hecho, uno de los primeros 
pasos del procesamiento es volcar lo que queremos procesar, este archivo de datos, sobre la memoria RAM de la infraestructura. Acá 
tenemos una infraestructura de 16 GB de RAM, 10 MB entran en esos 16 GB de RAM, casi no va a ocupar ningún espacio, entonces, esta 
laptop lo puede procesar. Mandamos a ejecutar el script sobre esta infraestructura y tenemos el archivo de reporte o la red neuronal 
o lo que el negocio nos había pedido y ahí está tu reporte y negocio ya te lo puedes llevar y hacer lo que tu gustes. Negocio te 
dice: “ … perfecto … “, hasta ahí vamos bien. Viene negocio el día siguiente y ahora te da otro archivo de datos, este es un poco 
más pesado, ahora tenemos un gigabyte de información y te dice lo siguiente: “ … no hay ninguna modificación en la necesidad que yo 
he tenido como negocio, las reglas de procesamiento son las mismas, es decir, tu script es el mismo, solo que ahora el archivo que 
lee tu script tiene más información, si en archivo anterior habian 100.000 registros, pues en el nuevo archivo hay 1.000.000 de 
registros, tenemos un gigabyte de datos, entonces, es tan simple como ahora que tu proceso apunte a este nuevo archivo, vamos a darle 
al botón de ejecutar y ¿que sería lo primero? pues el proceso volcará el archivo de datos en la memoria RAM, un giga de datos, pues, 
si entran en 16 GB de RAM, así que, la laptop lo va a poder procesar y listo tenemos el nuevo output y negocios se lleva ese output. 
Al tercer día viene nuevamente la persona de negocio y te dice lo siguiente: “ … ahora te voy a dejar otro archivo de datos, la lógica 
de negocio va a ser la misma, solo que ahora tenemos 10 GB de información a procesar … “  y tú dices: “ … de acuerdo, ya estamos casi 
en el límite de la infraestructura, pero, lo va a poder procesar de todas maneras, esos 10 GB entran en esos 16 GB de RAM, de hecho, 
sobra algún espacio en la memoria RAM para que el sistema operativo se ejecute y todo lo que tu laptop tenga para funcionar y va a ir 
lento, probablemente, porque ya es más información, se va a demorar probablemente más pero lo va a poder procesar … “ y listo, tenemos 
la resultante y negocios se lo puede llevar. El problema vendrá el cuarto día, viene negocio y ahora te da un archivo de 50 GB de 
información, entonces, el primer paso de tu procesamiento es volcarlo a la memoria RAM, pero 50 GB de datos no entran en 16 GB de RAM, 
por lo tanto, esta infraestructura es demasiado pequeña para el procesamiento, ¿qué es lo que va a pasar? pues al momento de hacer el 
volcado en la memoria RAM de lo que queremos procesar se va a producir un desbordamiento de memoria RAM o en términos más simples, 
pues la infraestructura era demasiado pequeña para la volumetría del proceso, ahí tenemos el primer problema. ¿Cómo se solucionaba 
esto de manera tradicional? conceptualmente es fácil de definir una solución, podríamos decir lo siguiente: “ … oye acá tenemos un 
problema de volumetría, son 50 GB de información que van a ser procesados en mi laptop, que solo tienen 16 GB de RAM, entonces, pues 
eso va a producir que haya un desbordamiento de la memoria, pero no pasa nada, porque, yo conozco un patrón de diseño de procesamiento 
por partes, vamos a partir el archivo en 5 partes de 10 GB y vamos a hacer que el proceso vaya procesando de 10 en 10, leemos 10 GB, 
los volcamos en la RAM, eso sí entra aquí y tenemos una resultante parcial. Luego liberamos la memoria RAM y leemos los siguientes 
10 GB, volvemos a cargarlos el proceso hace lo que tiene que hacer y tenemos la segunda resultante y así sucesivamente. Esto es lo que 
se conoce como una implementación por partes, ¿funciona? claro que sí, el problema es que, recordemos que en el proceso que habíamos 
implementado teníamos la lógica de negocio, la necesidad que negocios tenia, el problema ahora está en que esto de partir el archivo 
en varias partes e ir procesándolos por partes, ir juntando cada parte e ir liberando la memoria RAM entre cada procesamiento, todo eso 
no es algo que se haga de manera automática, vamos a tener que modificar el script y dentro colocarle lógica de procesamiento por 
partes. Así que habrá que modificar el script para que ahora se comporte de esta manera. ¿Cuál es el problema de codificar con lógica 
de procesamiento por partes? que depende de la dificultad algorítmica, ¿a qué me refiero? recordemos que las necesidades de negocio 
pueden ser variables, lo más simple que nos podría pedir es un pequeño reporte, pero entre eso hay muchas otras cosas, por ejemplo, 
tal vez nos pida un flujo de limpieza de datos que tenga miles de líneas de código o podría pedirnos, por ejemplo, programación 
funcional, que de esto vamos a hablar más a detalle en la próxima sesión, pero, básicamente es crear funciones personalizadas que 
implementen reglas muy complejas o, por ejemplo, podríamos hacer un flujo de tiempo real o podría pedirnos procesar data 
semiestructurada o no estructurada o podría pedirnos algún tipo de modelo analítico o de deep learning, entonces, pues las necesidades 
de negocio pueden llegar a ser muy complejas. Dependiendo de la complejidad algorítmica que tenga esa necesidad, mientras más complejo 
sea el algoritmo, pues más compleja es la implementación de la lógica de procesamiento por partes, no todas estas necesidades se 
resuelven partiendo del archivo en varias partes, porque, a veces una parte para ser procesada depende de analizar otras partes, 
entonces, ahí es donde viene la dificultad. Por eso, conceptualmente, la lógica de procesamiento por partes es fácil de entender, el 
problema es llevar a código. Eso haría que tu proceso implementado no solamente tenga la necesidad de negocio, sino, que adicionalmente 
tú como desarrollador tienes que meter en el script por fuerzas, porque, la infraestructura es muy pequeña, esta lógica de 
procesamiento por partes. Ahora, ampliemos esto a la vida real, dentro de una empresa no va a haber solo un proceso, las empresas 
literalmente tienen miles de procesos, entonces, ¿qué es lo que pasa? cada uno de estos procesos inicialmente pueden hacer con 
naturaleza de procesar poca cantidad de información, quizá 10 MB, pero en el tiempo es probable que la volumetría aumente, llegará un 
punto en que la infraestructura del proceso es demasiado pequeña, entonces, un desarrollador tendrá que darle mantenimiento al script 
para agregarle la lógica de procesamiento por partes, pero este script puede ser muy complejo, de hecho, podríamos asumir que es el 
tipo de proceso más simple, una reportería, pero recordemos que en la vida real estos reportes tienen muchas líneas de código y si ya 
de por sí es difícil que el mismo desarrollador que hizo el código le dé mantenimiento al código, imagínense que esta persona se va de 
la empresa y ahora viene otra persona para darle mantenimiento a un código que él no conoce, tiene que haber una etapa de análisis que 
le va a consumir 2 o 3 días, una vez que entienda el código, tiene que plantear la estrategia de procesamiento por partes y luego de 
plantearla tiene que implementarla, quizás no funcione bien y luego tiene que tunearla y todo eso se traduce en tiempo y negocio 
dice: “ … oye ya está mi proceso que iba a procesar 50 GB de datos? … “ y la respuesta es: “ … eh no, lo que pasa es que todavía 
estamos viendo cómo lo vamos a modificar para que sea eso posible y esto no para un proceso, sino, para todos los procesos que tenga 
la empresa, por lo tanto, las soluciones que se implementen con lógica de procesamiento por partes no son escalables y esta es la 
palabra clave, no es una solución escalable, es decir, no puede crecer en el tiempo. Tradicionalmente estos eran los problemas que los 
desarrolladores tenían y ¿cómo se solucionaba? la manera más fácil era: “ … mira negocio, me estás dando un archivo de 50 GB de datos 
y mi laptop pues no va a poder procesar eso, qué te parece si compramos un servidor empresarial linux que tenga 100 GB de memoria 
RAM … “ ahora sí los 50 GB de datos pueden entrar en este nuevo servidor, entonces negocio te dice: “ … de acuerdo … “. Para no 
complejizar el desarrollo de los procesos, el proceso ya no se va a ejecutar en tu laptop, ahora se ejecutará en un server empresarial, 
así que tendríamos lo siguiente: 50 GB de datos, la infraestructura ya no sería tu laptop, sino, que sería un server de 100 GB de RAM, 
colocaríamos ahí dentro el proceso con la lógica de negocio que has implementado, el primer paso es volcar el archivo a la RAM y ahora 
sí entra y no hay nada de implementaciones de lógica, de procesamiento por partes, le damos al botón y tenemos el reporte y negocios se 
lo lleva. Negocio te dice: “ … perfecto, con este server ahora si está funcionando todo bien … “. El problema viene a continuación, al 
día siguiente viene negocio y te dice: “ … tenemos más volumetría para ese proceso, ahora son 90 GB de datos, de acuerdo estamos casi 
en el límite, el servidor lo va a poder procesar y listo, aquí está tu resultante y negocios se la lleva, Al tercer día, viene negocio 
y te dice: “ … oye gerencia ahora nos ha dado un archivo de 300 GB de datos y lo quiere para ahora … “, vamos a procesarlo, la 
respuesta es que nuevamente estamos con el problema de límite de la infraestructura, pues esos 300 GB no entran en los 100 GB de este 
servidor en su memoria RAM y nuevamente, lógica de procesamiento por partes que ya sabemos que se traduce en tiempo y no es algo 
seguro, porque, hay que estar haciendo evaluación dependiendo de lo que se haya codificado y también sabemos que la empresa tendrá 
literalmente miles de procesos y tendremos problemas en todos esos procesos y, por lo tanto, nuevamente estamos en soluciones que no 
son escalables. Entonces ¿qué es lo que pasó? vamos a solucionar esto con un concepto que existe desde los años 60, hemos escalado 
desde una infraestructura de una computadora común a una infraestructura de un servidor empresarial, pero aun así esto se queda 
pequeño, así que vamos a escalar al siguiente nivel de infraestructura, es un concepto que existe desde los años 60, realmente no es 
nada nuevo, el de clúster computacional. ¿Qué es un clúster computacional? un conjunto de servidores que trabajan de manera paralela, 
para resolver un mismo problema. Realmente la arquitectura de un clúster es más compleja ya aprenderemos de esto más adelante cuando 
hayamos hecho codificaciones sobre uno, de hecho, hay más detalles técnicos aquí, pero todo esto ya lo vamos a hacer directamente en 
código. Por ejemplo, podríamos tener un clúster de 10 servidores, para simplificar los cálculos, porque de hecho de cada servidor 
importa el disco duro, la memoria RAM y los núcleos de CPU, por ahora, solamente hablemos de memoria RAM. Asumamos que cada uno de 
estos servidores tiene 100 GB de memoria RAM, cómo tenemos 10 de estos servidores esto se traduciría a tener 1.000 GB de RAM, es decir, 
tendríamos 1 TB de memoria RAM en nuestra disposición para ejecutar lo que queramos. Entonces, en esta nueva infraestructura tenemos 
esa potencia computacional, ahora viene negocio y te dice: “ … de acuerdo, aquí está mi archivo de 300 GB de datos, ya hemos comprado 
un clúster, ahora sí tienes la potencia para procesar  este archivo … “ de acuerdo, aquí tenemos este proceso, el cual tiene el script 
con la lógica de negocio que has desarrollado en tu lenguaje de programación, el proceso leerá el archivo de datos y ¿qué es lo que 
tendrá que hacer? no puede volcar el archivo de datos en un solo servidor, ya que, cada uno tiene 100 GB, así que habría que partir el 
archivo de datos en 3 partes, 100 GB en un server, 100 GB en otro server y 100 GB en otro server y luego de manera paralela mandar a 
ejecutar el script en estos 3 servidores. Entonces, conceptualmente vemos que esto funcionaría, pero realmente aquí nos estamos creando 
otro problema, esto de distribuir cada parte del archivo entre diferentes nodos del clúster y mandar a ejecutar el proceso entre 
diferentes nodos del clúster y luego las resultantes parciales se unan en uno de los nodos no es algo que se implemente de manera 
automática, tú como desarrollador vas a tener que agregar en tu script la lógica de distribución de carga de trabajo, eso es lo que se 
implementa en los clusters. Para poder distribuir la carga de trabajo sobre un clúster, es decir, sobre sus servidores, tú como 
desarrollador tendrías que implementar esa lógica, ahora eso tampoco es tan simple de hacer. ¿Qué problemas comunes tienen los 
clusters? por ejemplo, digamos que el proceso se toma 3 horas en ejecutarse, y lleva 2 horas y luego de esas 2 horas, pues hay un 
parpadeo en la red o simplemente el equipo que está encargado de esta infraestructura no gestionó bien este servidor y por alguna razón 
perdemos este server, entonces, también perderíamos la parte que se estaba procesando en ese servidor, ¿qué debería hacerse en ese 
caso? vamos a agregar al proceso una lógica de gestión de excepciones, vean como cada vez el script se hace más complejo, negocio te 
había pedido algo, pero por problemas técnicos hay que colocarle todavía más código que nada tiene que ver con la necesidad de negocio, 
realmente no es un código que aporte valor al negocio y esto va a ser muy importante, se supone que la lógica de negocio es el código 
que representa esa necesidad de negocio, pero por impedimentos técnicos pues hay que ponerle más cosas al script, por ejemplo, la 
lógica de gestión de excepciones, tendríamos que programar: “ … oye si una parte del proceso cae en un servidor buscamos un server que 
esté disponible y relanzamos esa parte en ese otro servidor … “ y para eso, por supuesto hay que programar, tampoco es tan fácil de 
programar. Por lo tanto, digamos que un clúster nos da una infraestructura escalable, es decir, si queremos tener más potencia, pues, 
simplemente instalamos más servidores, pero eso no nos libera de los problemas que hay en la parte de la programación, entonces, ¿qué 
es lo que pasó? este concepto de clusters existe desde los años 60 y así se utilizaba, el problema es que nuevamente no eran procesos 
escalables, lo que si era escalable era la infraestructura, más potencia más nodos, pero el proceso no, porque, mientras más datos 
venían pues habría que agregar más código a la necesidad de negocio, código técnico que nada tenía que ver con el problema de negocio. 
Este problema se empezó a ser frecuente alrededor de los años 2004 y 2006, las grandes empresas como Google, Microsoft, Oracle y muchas 
otras empresas que eran las top tecnológicas de esos años empezaban a tener estos problemas para sus procesos y decían: “ … oye yo en 
mi empresa pues tengo ya varios procesos y en estos procesos cada vez estamos procesando más volumetría y ya se está volviendo 
inmanejable la codificación y mantenimiento de esos códigos … “ todas estas empresas tenían eso en común, así que dijieron: “ … ¿qué 
podemos hacer para solucionar este problema? Se sentaron y dieron una definición. Vamos a definir cómo debería ser la tecnología ideal 
que resuelva todos estos problemas. Lo primero que vamos a hacer es, de acuerdo, el concepto de cluster si nos sirve, ahí ya tenemos 
resuelta la escalabilidad de la infraestructura, ya que, si necesitamos más potencia pues simplemente compramos más servidores y los 
ponemos dentro del cluster, así que, vamos a basarnos en el concepto de infraestructura de clúster, ya que, aquí podemos escalar con 
tantos servidores como queramos. Lo otro que definieron es también lo siguiente, el problema es que cuando un desarrollador implementa 
su lógica de negocio, los desarrolladores no ven al clúster como un todo, sino, como un conjunto de servidores. ¿Qué les parece si 
hacemos lo siguiente? vamos a definir un clúster especial, el cual haga que los desarrolladores lo vean como si fuese un todo, ya no 
como un conjunto de servidores. ¿Eso qué significa? digamos que tenemos 10 servidores de 100 GB de RAM cada uno, entonces, el 
desarrollador va a ver al clúster como si fuese un súper servidor de 1000 GB de RAM en total, entonces, pues el desarrollador ya no ve 
al clúster como un conjunto de servidores separados, si no, como un gran súper servidor que tiene toda la potencia computacional que 
sería la suma de las potencias de los servidores que hay dentro. Entonces, dijeron: “ … de acuerdo … “ esa tecnología debería funcionar 
de esa manera, el desarrollador no va a haber al clúster como un conjunto de servidores, sino, como un único gran súper servidor. 
Entonces él solamente se dedica a programar lo que negocio les haya pedido. Ahora, vamos a ejecutar algo, aquí tenemos el archivo de 
300 GB de datos y aquí mandaremos a ejecutar el proceso, lo primero que hay que hacer es, si se dan cuenta, un clúster va a tener mucha 
potencia y quizá el día de hoy no necesitemos toda la potencia del clúster para el procesamiento, por ejemplo, podríamos reservar 
solamente 300 GB de RAM, entonces, vamos a reservar un 30% de la potencia del clúster. Esta tecnología que estamos definiendo y por 
supuesto aún no existe qué le debería de permitir al desarrollador, decir lo siguiente: “ … a ver ¿cuánto vamos a procesar el día de 
hoy?, 300 GB de datos, entonces, el día de hoy vamos a reservar el 30% de la potencia del clúster y el otro 70% estará libre para que 
otros desarrolladores trabajen. Al día siguiente negocio viene y te deja un archivo de 400 GB de datos y te dicen: “ … la lógica de 
negocio es la misma no vas a cambiar ni una línea en tu código, solo que ahora hay más datos, de acuerdo, dame 1 segundo, el día de 
ayer en el porcentaje de reserva del clúster había puesto 30%, porque, me diste 300 GB y eso es el 30% de esta potencia del cluster, 
pero el día de hoy me estás dando 400 GB y voy a necesitar más potencia, dame 1 minuto para adaptar el script … “. Ahora el script que 
es lo que va a decir: voy a ir a la línea de reserva de potencia del clúster y le diré ahora quiero que diga “dame un 40%” … “, 
entonces la propia tecnología que gestiona el clúster debería de reservar ese 40%, quizá de estos cuatro servidores o quizá de estos 
cuatro de aquí o quién sabe de qué servidores, eso ya sería trabajo de la propia tecnología que gestiona el cluster. Para simplificar 
digamos que son estos cuatro servidores, entonces el desarrollador antes de mandar a ejecutar solo debería de modificar una línea, la 
línea que indique cuánta potencia del clúster quiere reservar, en este caso “ … ya dame 1 segundo y lo pongo en 40% … “. Mañana viene 
negocio y te deja un archivo de 200 GB y te dice: “ … ah ok dame 1 segundo voy a entrar a mi script y ahora el porcentaje de reserva 
es un 20%, porque, solo necesitamos eso … “, entonces, esta tecnología que debía cumplir como segundo requisito, que fácilmente al 
desarrollador le permita reservar cierta potencia del clúster en función de la volumetría que este va a procesar. Por supuesto, que 
aquí lo estoy poniendo como algo tan simple donde voy una línea y la modifico, por ejemplo, de 40% a 20%, realmente no es así, hay más 
código que colocar, pero es algo que toma de 5 a 10 minutos en configuración y ya no te va a tomar varios días o semanas en adaptar. 
Una vez que hemos reservado la potencia del clúster, digamos 400 GB el día de hoy, dame el 40%, ¿qué es lo que debe hacer esta 
tecnología que gestiona el cluster? el tercer punto es que, de acuerdo, se seleccionó estos cuatro servidores, la propia tecnología 
del clúster tomará la que tú hayas codificado y la distribuirá entre esos servidores que ha seleccionado, igual con el archivo de 
datos, hará esa distribución de carga de trabajo de manera automática, ejecutará esos procesos en esos servidores y en alguno juntarán 
las resultantes parciales y te dirán listo ya terminé de procesar. Vean entonces que una tecnología de este tipo nos facilitaría mucho 
el desarrollo, porque, ¿qué es lo que haríamos? vamos a codificar el proceso con 10 MB que negocio me ha dado de ejemplo, ahí 
probablemente con reservar solo un servidor sería suficiente y trabajamos así. Terminamos y le decimos: “ … listo negocio ya tenemos 
esa necesidad del reporte que tú me habías pedido, ahora sí dame la volumetría real … “ negocio viene y te dice: “ … vamos a procesar 
300 GB, en ese caso necesitamos un 30% de la potencia del cluster … “ que ya verá el clúster de qué servidores saca eso y la propia 
tecnología de cluster distribuirá tu proceso en esos servidores, es decir, implementa la lógica de distribución de carga de trabajo, 
pero, ya de manera automática. Si al día siguiente esa volumetría crece, pues vamos a la sección de reserva de recursos computacionales 
y le asignamos la nueva reserva que necesitemos en función de esa volumetría. Y ¿qué pasa si un día negocio te deja 1500 GB de datos a 
procesar? pues el clúster sería pequeño, pues, no hay problema vamos a traer 5 servidores nuevos, listo, ya tenemos 1500 GB, vamos al 
script , ¿cuánto quiero? pues el 100% de la potencia del clúster, o sea, pues 1500 GB de memoria RAM que tenemos y listo todo se hace 
de manera automática. Vean entonces que una tecnología de este tipo facilitaría mucho la escalabilidad de los procesos, el proceso, lo 
que tu implementes, ya no va a depender de la volumetría, ya que, esta propia tecnología en función de la reserva que tú hagas pues 
paraleliza tu código sobre esos servidores, entonces, para ti como desarrollador te es más fácil, codificas una vez y luego si viene 
más volumetría pues reservas más potencia del cluster  y el propio clúster distribuye su carga de trabajo de manera automática. 
Entonces, estas grandes empresas como Google, Microsoft y otras en esos años dijeron: “ … perfecto eh esta es la tecnología que nos va 
a ayudar … “, por supuesto que todo esto lo hicieron por medio de definiciones, en el día uno no existía nada y esto dijeron: “ … y 
¿qué nombre le ponemos a esta tecnología? como está relacionada con la volumetría de los datos, es decir, mira puede que vengan más 
datos, entonces está relacionado con el procesamiento de grandes cantidades de datos o en inglés Big data y por eso decidieron ponerle 
este nombre a las tecnologías que nos permiten tener procesos que puedan escalar en el tiempo en función de más volumetría. Dijeron: 
“ … perfecto, entonces, ya no vamos a trabajar con clusters clásicos en donde hay que distribuir manualmente la carga de trabajo, sino, 
que vamos a trabajar sobre clusters de big data … “, es decir, clusters que cumplan todo el concepto que hemos definido. Así que todas 
estas grandes empresas empezaron a implementar este concepto, por ejemplo, Google tomó este pliego conceptual e hizo su propia 
implementación para gestionar cluster de big data conocida como GFS y así pues varias grandes empresas tomaron el concepto e 
implementaron esas tecnologías que definían esos conceptos. Algunas de ellas eran propietarias, es decir, solo Google la tenía o solo 
Microsoft, no las podías descargar en internet, pero como todo en la vida también habían proyectos de software libre y de hecho habían 
muchos en esos años. Uno de ellos era HADOOP. En su momento habían muchos proyectos de software libre que implementaban el concepto de 
Big data, pero de todos ellos el que era más fácil de utilizar y el que mejor usaba eficientemente los recursos del clúster era HADOOP, 
entonces, como HADOOP era fácil de utilizar y como usaba muy bien los recursos del clúster, se convirtió en el estándar de software 
libre para gestión de clúster de Big data y se popularizó mucho en la comunidad, tanto así que incluso hasta el día de hoy, Hadoop es 
el estándar de gestión de clusters para Big data. Hasta ahí entonces bien, ya tenemos una tecnología que va a implementar todos estos 
conceptos. Pero, y ¿qué tiene que ver Hadoop con Spark? vamos a ese punto. Entonces ya tenemos una tecnología que va a gestionar un 
clúster para que se comporte como un cluster de big data. ¿Cómo funciona Hadoop? Hadoop tiene cuatro componentes, de esos cuatro 
componentes uno de ellos al día de hoy se reemplaza por Spark, pero vamos a entender bien la arquitectura que se maneja por detrás. Lo 
primero que hay que saber respecto a Hadoop es que gestiona los servidores de un cluster, pero ¿qué es un servidor? desde un punto de 
vista de procesamiento hay 3 recursos computacionales que nos importan de los servidores: su capacidad de disco duro, la cantidad de 
memoria RAM que tenga el servidor y los núcleos de CPU que tenga este servidor. Por ejemplo, hablando de servidores linux empresariales 
clásicos podríamos tener 10 TB de disco duro, 100 GB de memoria RAM y 40 núcleos de CPU. Aquí también hay un punto técnico que todavía 
lo vamos a ver en código, ya que, aún faltan varios conceptos, pero luego de haber codificado esos conceptos se va a entender todavía 
mucho mejor. Cuando hablamos de la CPUs hay 3 niveles, una cosa son las CPUs, otra cosa son los núcleos de CPU, por ejemplo, te puedes 
comprar tu laptop y es multinúcleo, tiene 2 CPU de cuatro núcleos cada CPU y tendrías 8 núcleos de CPU en total. Pero hay un término 
más, que es el que se utiliza para la codificación que son las Virtual CPU (vCPUs) que son núcleos de CPU emulados sobre una misma CPU. 
De esto todavía hablaremos más a detalle cuando hagamos tuning computacional en el código, para paralelizar aún más los procesos. Más 
adelante van a entender que hay un factor de paralelización del código, podemos hacer con la misma cantidad de CPUs el código hasta 20 
o 30 veces más rápido, dependiendo del factor de hyper-threading, entonces, ya hay todo un detalle  técnico, pues, que sería demasiado 
verlo en este momento, el objetivo del día de hoy es una introducción teórica de todos estos conceptos. Así que ahora voy a hablar que 
simplemente tenemos 40 núcleos de CPU, aunque el término correcto más adelante sabrá que son vCPUs. Esa es la potencia de este servidor 
y para el disco duro se tiene 100 GB de RAM. Cuando construyamos un clúster podemos poner diferentes servidores, podremos poner otro 
servidor, con otra capacidad de disco duro, de memoria RAM y otra cantidad de núcleos de CPUs. Pero para simplificar la explicación 
vamos a asumir que pongamos 10 servidores y todos tienen estas características para que se pueda hacer con un cálculo de multiplicación. 
Recordemos que la facilidad que nos da un clúster de big data es que ya nos olvidamos si por detrás hay 20 servidores o 50 servidores, 
solamente nos interesan los números total. Como teníamos 10 servidores de 10 TB cada uno, pues en el clúster tendríamos 100 TB de 
almacenamiento en total para poder guardar todo lo que queramos. Respecto a la memoria RAM, ¿cuánta RAM en total tendría el clúster? 
pues lo mismo, son 10 servidores de 100 GB de RAM cada uno, pues la potencia total para procesar en la memoria RAM sería de 1.000 GB, 
es decir, 1 TB de memoria RAM. Y lo mismo pasaría con los núcleos de CPU, es como si tuviéramos solo una CPU, cómo había 10 servidores 
de 40 núcleos de CPU es como si tuviéramos un súper servidor de 400 núcleos de CPU. Tú como desarrollador vas a ver al clúster con esta 
capacidad:

-	100 TB de almacenamiento en disco duro
-	1 TB de memoria RAM
-	400 núcleos de CPU

Es como si fuese solo un server con esa potencia, aunque por detrás ya sabemos que todo está distribuido en diferentes servidores. Para 
que esto sea posible también sabemos que Hadoop es el encargado, es quien hace esta configuración, permite que el desarrollador vea al 
cluster como si fuese un gran super servidor con esa potencia, de hecho, cuando ustedes escuchen que en China o en Estados Unidos o en 
Europa, han inventado un súper servidor de 96 TB de memoria RAM, realmente están haciendo referencia a un clúster de big data. ¿Cómo 
hace posible esto Hadoop? Hadoop tiene diferentes módulos. Tiene un módulo para gestionar los discos duros de los diferentes servidores, 
ese módulo es conocido como HDFS, qué significa HADOOP DISTRIBUTED FILE SYSTEM o en español el sistema de archivos distribuidos de 
Hadoop, entonces, ese es el módulo que te permite ver a todos los discos duros como si fuese solo un gran disco duro. Luego, también 
recordemos que para desarrollar que tenemos que hacer, hay que separar un 50% de la potencia del clúster o un 20% o un 30% o lo que se 
necesite. Hadoop tiene un módulo conocido como YARN, y significa YET ANOTHER RESOURCE NEGOTIATOR o en español si otro negociador de 
recursos. ¿Qué es lo que permite hacer el modelo de YARN? YARN gestiona la memoria RAM y los núcleos de CPU y él es el que permite 
hacer esa reserva, de hecho, HDFS en la nube, por ejemplo, algunos lo reemplazan con el S3 de Amazon, entonces, HDFS al día de hoy 
puede que sea opcional en algunos momentos, pero por ejemplo, YARN es tan bueno que hasta el día de hoy se utiliza, una configuración 
de Spark con  YARN es la manera clásica de procesamiento, aunque también es modular lo podemos reemplazar por otro gestor de recurso 
pero al final hace lo mismo, reserva cierta potencia del clúster. ¿Qué nos está faltando? Sobre HDFS, el sistema de archivos 
distribuidos de Hadoop, vamos a subir los archivos que queremos procesar, negocio te deja un archivo de 300 GB de datos, bien, lo 
tendremos que poner aquí, ahí está el gran archivo que queremos procesar. El sistema de archivos distribuidos sirve para eso para dejar 
los archivos grandes que negocio te dejan para procesar. ¿Cuál es el segundo paso? una vez que sobre el sistema de archivos haz dejado 
los archivos que necesitamos procesar, lo siguiente es procesarlos, pero no necesitamos el 100% de la potencia del clúster, quizá el día 
de hoy sólo necesitemos un 20% de la potencia del cluster. ¿Con qué módulo reservamos eso? Con YARN, de alguna manera en código a YARN 
le tenemos que decir: dame el 20% de la potencia del cluster y listo ya tenemos reservado, que acá sería, por ejemplo, 200 GB de memoria 
RAM, y aca serían 80 núcleos de CPU. Una vez que hemos hecho la reserva ahora sí vamos a ejecutar nuestro código. ¿Tú como desarrollador 
cómo vas a codificar? como dijimos en un principio existen diferentes lenguajes de programación, por ejemplo, existe Scala, R, .net o 
Python o Java, hay muchísimos lenguajes. Estos lenguajes ya sabemos que no se pueden paralelizar en los 3 o 4 servidores que hayamos 
reservado, sino que, en el medio tiene que haber un motor que distribuya la carga de trabajo de manera automática, ese motor recibirá la 
instrucción del lenguaje y sobre los servidores que hayas reservado, pues ejecutará esa instrucción distribuyendo la carga de trabajo 
de manera automática. Hadoop tiene un módulo conocido como MapReduce que es el gestor de distribución de carga de trabajo sobre el 
clúster. ¿Cuál es el objetivo de mapReduce? recibir la sentencia en el lenguaje que ya es codificado y distribuir la carga de trabajo 
de esa sentencia entre los servidores que hayas reservado. Cuando tú instalas, Hadoop, por defecto, para gestionar un clúster por 
defecto te vas a encontrar con esos 3 módulos: HDFS para la gestión de discos duros, YARN para la gestión de memoria RAM y CPU y 
mapReduce para la gestión de distribución de carga de trabajo automática. Este tipo de configuración se popularizó desde el año 2006 
hasta aproximadamente el año 2010, era el estándar. Todo el mundo que quería hacer big data, al menos con software libre, descargaba 
esto y pues aplicaba esta configuración sobre su clúster y funcionaba. Pero ¿cuál era el problema? mapReduce era muy bueno, pero nació 
pues pensando en otro tipo de necesidades para esas épocas. El problema es que desde el año 2010 en adelante comenzaron a surgir otro 
tipo de necesidades, por ejemplo, se hablaba mucho del procesamiento analítico, y por ejemplo, se habla mucho de 2 tipos de 
procesamiento analítico: de machine learning o de Deep learning. También se estaba hablando mucho de bases de datos NoSQL y esto pues 
en sí no es un curso de NoSQL, porque, ahi literalmente tendríamos que ver gestores de bases de datos y  nosotros estamos más 
interesados en procesar con Spark y Scala, pero para ponerlo en términos simples si hablásemos de gestores NoSQL, podemos dividir los 
datos en: datos estructurados como un excel o una tabla de base de datos y datos semiestructurados como archivos JSON y archivos XML y 
datos no estructurados como fotografías, videos, audios. Entonces, ya en estos años pues habían más necesidades, inicialmente Hadoop 
nació con la filosofía de hacer procesamiento de datos estructurados que crecen en el tiempo y en esta época pues estuvo bien, pero del 
2010 en adelante ya habían otros tipos de necesidades y mapReduce el cual era el motor que distribuía carga de trabajo, estaba orientado 
a este tipo de distribución, no a estas nuevas necesidades y de hecho empezaron a salir más, procesamiento en tiempo real, procesamiento 
de redes neuronales, flujos de ETL paralelizables y cada vez más cosas que ya pues mapReduce no daba la talla, no porque sea un mal 
motor, sino porque, simplemente nació con una filosofía de esa época, pero ya era otra época y habían más necesidades. ¿Qué es lo que 
pasó ese año? dijeron HDFS y YARN hasta el día de hoy funcionan bien, pero, mapReduce ya no está funcionando tan bien, porque, ahora hay 
otras necesidades. Así que nuevamente se repitió la historia, muchos proyectos de software libre empezaron a construir un nuevo módulo 
para Hadoop, para reemplazar a mapReduce, entonces pues había muchas propuestas, algunas como todo en la vida eran buenas y otras no 
tan buenas. De todas estas propuestas la que mejor reemplazaba el módulo de mapReduce, era fácil de utilizar y a la vez gestionaba bien 
los recursos del clúster era Spark. Así que desde el 2010 en adelante comenzaron a sacar el módulo de mapReduce para la distribución de 
carga de trabajo y vamos a poner un mejor modulo, que en la comunidad es el que mejor ha demostrado comportarse, al menos, en software 
libre es Spark. Y ahí es donde entra nuestro punto de interés y dijeron: “ … bien, ahora cómo motor de distribución de carga automática 
usaremos Spark … “. ¿Cómo programamos sobre este motor? pues sigue la misma filosofía que el motor antiguo de mapReduce, usa el lenguaje 
que mejor se adapte a la necesidad. Este motor de carga de trabajo tiene APIs para diferentes lenguajes, puedes usar R,  Scala, .net, 
Python, Java, nodeJS, incluso hasta hay módulos para PHP, entonces usa el lenguaje que a ti te guste, porque no es que haya un mejor 
lenguaje, ya sabemos que cada lenguaje se adapta a cierta necesidad y Spark se empezó a popularizar y se convirtió en el estándar de 
distribución de carga de trabajo sobre los clústers. Al día de hoy esta es la configuración que se utiliza para la gestión de un 
clúster. Ya hemos llegado al punto en donde hemos colocado Spark. Ahora ¿qué es lo que pasó? pues Spark nos permite hablar con el 
clúster con diferentes lenguajes de programación. Spark solo es el motor de procesamiento, no trae un sistema de archivos distribuidos, 
necesita de uno, tampoco trae un gestor de recursos, de hecho, trae uno incrustado pero es solamente para probar, no se recomienda usar 
en modo productivo, entonces, necesita también de un gestor de recursos que permita separar la potencia del clúster. Así que Spark sí o 
sí,  tiene que apoyarse en otras tecnologías, pero por un momento vamos a aislar solo este módulo. ¿Qué es lo que pasa? pues en esas 
épocas todo el mundo usaba sus lenguajes de programación favoritos Scala, R, .net, Java, Python, entonces, ¿qué dijieron?: 

En esta filosofía del mundo del big data hay 2 grandes tipos de procesos que podemos implementar en tecnologías de big data, todos estos 
procesos de reportería, programación funcional, redes neuronales, Deep learning, machine learning, programación real time, procesamiento 
semi estructurado, hay muchos tipos de procesos, pero los podemos agrupar en 2 grandes procesos: procesos de data engineer o en español 
de ingeniería de datos o procesos de data scientist o en español de ciencia de datos. Por ejemplo, los procesos de tipo funcional o de 
real time o procesos de programación semi estructurada o no estructurada, entran dentro de la clasificación de procesos de ingeniería 
de datos. Los procesos de ciencia de datos, por ejemplo, los procesos de ingeniería datos están orientados a la preparación de los 
datos, a leerlos, a limpiarlos, a modelarlos, a estructurarlos y a optimizar cruces de datos. En cambio los procesos de ciencia de 
datos están orientados a descubrir patrones dentro de los datos, por ejemplo, los procesos de machine learning o Deep learning. 
Entonces se dieron cuenta de esto, bien podemos agrupar todos los procesos en dos grandes tipos o de ingeniería de datos o de ciencia 
de datos, en el primero preparamos los datos y en el segundo se evalúan. 


Entonces, bien y ¿qué lenguajes de programación se adaptan mejor a este tipo de procesos?
-----------------------------------------------------------------------------------------

Haciendo una evaluación ya muy técnica, vamos a poner todo esto en los servidores, vamos a ejecutarlo y vamos a ver qué tanta RAM y que 
tanta CPU consumen y si podemos alcanzar más nivel de paralelización, entonces en esencia realmente es un poco más complicado, pero lo 
estoy simplificando, y se dieron cuenta de lo siguiente: el motor de Spark está implementado en Scala, cuando crearon Spark dijeron ¿en
qué programamos? en Scala. 


¿Cuál es la ventaja que tiene Scala?
------------------------------------ 

Scala tiene una sintaxis muy simple, Python, por ejemplo, de la gran mayoría de lenguajes de programación que existen, Python destaca 
en que su sintaxis es muy simple, el problema es que no gestiona también los recursos de los servidores, entonces, dijeron: el lenguaje 
Java es muy bueno en la gestión de recursos, podríamos crear Spark en el lenguaje de Java, el problema es que Java tiene una sintaxis 
muy compleja que dificultaría que este proyecto cada vez aumente más, así que vamos a utilizar un fork de Java conocido como Scala. 


¿Qué es lo que permite Scala?
----------------------------- 

te permite codificar con una sintaxis muy parecida a Python, pero que cuando le das al botón de compilar, se compila en código Java y 
por lo tanto se ejecuta sobre la máquina virtual de Java, es decir, que cuando Java recibe las líneas de comandos, Java ni cuenta se da 
que por detrás todo se implementó en el lenguaje de Java, sino que, Java lo entiende como si fuera código Java. Por eso es que Spark 
fue implementado en Scala, Scala tiene una sintaxis parecida a Python lo cual lo hace fácil en su programación y al ejecutarlo puedes 
ejecutarlo sobre la máquina virtual de Java, es decir, aprovechamos la robustez que tiene Java. 

Entonces dijeron: “ … de acuerdo ya que Spark ha sido implementado en Scala, realmente todo debería ser implementado en Scala, tanto 
los procesos de ingeniería de datos como ciencia de datos, ya que habla de manera nativa con el propio motor de Spark, en cambio el 
resto de lenguajes pues tienen que tener un proceso de traducción que hará que vayan un poco más lento, ¿cuánto? dependiendo del tipo 
de proceso de un 10% a un 15% más lento, aunque a veces eso no se nota pero ese es el peor de los casos. De acuerdo, todo debería estar 
en Scala … “. Entonces esa sería la respuesta. Pero nuevamente vamos a la vida real ¿qué es lo que pasa? antes de que saliera todo esto 
del motor de Spark y del mundo del big data, ya las personas hacían procesos de machine learning y Deep learning, no las hacían en 
clusters de big data porque no existían, los hacían o en su computadora o en servidores o ellos mismos en un clúster paralelizaban los 
algoritmos analíticos implementando por cuenta propia esa lógica de distribución de carga de trabajo. Entonces antes del big data ya 
existía el mundo de la ciencia de datos, ya se hacían procesos de machine learning y deep learning, de hecho, por ejemplo, las redes 
neuronales existen desde los años 50, no son algo nuevo, ya eso se viene haciendo desde hace mucho tiempo. ¿Qué es lo que pasaba? en 
esas épocas en donde aún no existía el mundo del big data, el lenguaje que se popularizó para la implementación de procesos de ciencia 
de datos, aún no en entornos de big data, simplemente no existía, fue Python, entonces ¿por qué? porque Python tenía una sintaxis muy 
flexible, era muy raro ver a personas haciendo ciencia de datos en Java, construir una red neuronal ya de por sí es muy compleja, ahora 
hacerlo sobre Java hace aún más complejo su desarrollo. Es por eso es que los científicos de datos antes de la era del big data, pues, 
todo lo hacían en Python. ¿Eso que hizo? que en la comunidad de Python pues habrían muchos frameworks de desarrollo para la parte de 
ciencia de datos sobre Pyrthon, literalmente tenemos miles de frameworks, entonces ya la comunidad de Python para la ciencia de datos 
por medio de librerías y frameworks que se fueron creando pues se hizo muy grande. Luego llegaron las tecnologías de big data y por 
supuesto que Scala también trae framework para ciencia de datos, pero ¿cuál era la diferencia? pues simplemente Python llegó antes, y 
ya habían muchos proyectos que ya estaban funcionando sobre código Python. Entonces Scala desde un punto de vista técnico era más 
rápido, pero la realidad es que no tenía tantos frameworks para el desarrollo de ciencia de datos como si los tenía Python. Así que 
¿qué dijieron? de acuerdo, si Scala hubiese salido en un principio antes que Python y toda la comunidad hubiera construido estos 
frameworks en Scala, bueno, pues ahí la recomendación hubiera sido para hacer ciencia de datos, hubiese sido Scala, pero como esa no 
fue la realidad, dijeron: “ … Ok, si quieres hacer ciencia de datos sobre un clúster que tenga Spark, pues, utiliza Python, no porque 
sea más rápido que Scala, sino porque, simplemente llegó antes y ya tiene muchísimos frameworks de desarrollo para para ciencia de 
datos … “. Por ejemplo, entre el más popular se encuentra Tensorflow, cosa que no se puede integrar tan fácilmente con Scala y en Python 
sí, entonces ya esto, por ejemplo, para hacer redes neuronales se utiliza mucho o para hacer temas de deep learning e inteligencia 
artificial, entonces, dijeron: “ … de acuerdo, la definición será la siguiente: para construir procesos de ingeniería de datos “Scala”, 
ahí siempre hay justificación, pero para construir procesos de ciencia de datos “Python”, por el simple hecho de que aquí hay más 
librerías y el desarrollo se hace más fácil. Eso no significa que no sea posible hacer este tipo de procesos en Scala, pero en la medida 
de lo posible hay que tratar de hacerlos en Python, porque, este tiene más librerías. Vean entonces cuál fue la definición, en resumen 
de acuerdo con Spark pues podemos hacer diferentes tipos de procesos, pero hablemos del mundo de big data, clásicamente se dividen en 
procesos de ingeniería de datos, es decir, de cualquier cosa que no sea analítica, cualquier cosa que no sea Machine learning o Deep 
learning va en Ingenieria de datos y aquí Scala es la mejor opción. Pero también pues tenemos los procesos de ciencia de datos, es 
decir, son 3 grandes tipos de procesos en la ciencia de datos: procesos de machine learning, procesos de deep learning y procesos de 
inteligencia artificial y aquí se utilizará Python porque ya hay muchísimos frameworks que fueron construidos pensando en ellos. 
Entonces de esta manera es como los entornos de desarrollo se dividen el trabajo, ingenieros de datos con Scala y científicos de datos 
con Python, aunque por supuesto nada impide que utilicen Python para ingeniería de datos y Scala para ciencia de datos. Y ¿qué pasa con 
el resto de lenguajes de programación, como por ejemplo R, SQL, .NET? pues ya están más acotados, por ejemplo, R está muy orientado a 
procesos estadísticos, digamos que quieres implementar un proceso para sacar una muestra poblacional según cierta tendencia o vamos a 
implementar la regla de distribución normal o distribución de una campana o distribución de chebyshev que son cosas ya estadísticas, 
literalmente esto le servirá a alguien que se dedique en la carrera de estadística. ¿.NET dónde se utiliza? digamos que estás en un 
ecosistema de Windows, donde absolutamente todo utiliza Windows, en este caso nos convendría las librerías de Spark para .NET, ya que, 
es una empresa que solamente se dedica a trabajar con SQL Server o con el ISD de Microsoft, entonces, como es el ecosistema de Windows 
quizás .NET se adapte mejor, entonces ya son casos un poquito más particulares. 

Ya entendemos el concepto de big data, estos son los tipos de procesos que se implementan en las empresas al día de hoy. El concepto de 
big data nos permite que los procesos sean escalables, eso ya lo habíamos visto. 


¿Que implica que sean escalables?
--------------------------------- 

Que la lógica de negocio que hayas implementado en tu script, va a recibir volumetría que crezca en el tiempo, sin la necesidad de 
modificar el script, solo la reserva computacional, hoy necesito el 10% del clúster y quizá mañana el 20%, y si el clúster se queda 
pequeño pues traemos más servidores. Eso significa ser escalable. 


¿Cómo implementamos el concepto de big data?
-------------------------------------------- 

Necesitamos un ecosistema tecnológico de big data que implemente todo esto, desde un punto de vista del software libre, Hadoop es el 
estándar del ecosistema el día de hoy, podemos descargar Hadoop desde su repositorio oficial e instalarlo en un cluster. También sabemos 
que Hadoop tiene 3 modulos: 

-	HDFS para la gestión de los discos duros 
-	YARN para la reserva de: dame cierta cantidad de memoria RAM y cierta cantidad de núcleos de CPU. 
-	mapReduce para distribuir la carga de trabajo sobre la reserva que hayamos hecho con el lenguaje que más nos guste, pero también 
    sabemos que al día de hoy esto ha sido reemplazado por Spark. 


¿A qué se debe? 
---------------

A que simplemente mapReduce nació con una filosofía de procesamiento estructural, pero al día de hoy hay otras necesidades más complejas 
que las empresas tienen y Spark fue la que mejor implementó esas necesidades. 

Lo siguiente que sabemos es y ¿qué lenguaje de programación usamos? también ya deberíamos estar en la capacidad de responder esa 
pregunta, existen muchos lenguajes pero hablando de procesos de big data básicamente los clasificamos en procesos de ingeniería de datos 
(data engineer) o procesos de ciencia de datos (data scientist). Lo ideal sería que ya que Spark fue construido con Scala y se compiló 
en Java, lo ideal sería hacer todo con Scala, pero lo que paso en la realidad es que como Python ya tenía muchas librerías de ciencia de 
datos pues Python se quedó con el mundo de la ciencia de datos y Scala con el mundo de la ingeniería de datos, pero también sabemos que 
nada impide usar Scala para hacer ciencia de datos o viceversa, Python para implementar procesos de ingeniería de datos, si el equipo 
lo conoce y tiene las librerías para hacerlo, adelante. 

Ahora escalemos a un siguiente nivel, ya hemos entendido dónde está el término big data, el término de Hadoop y el término de Spark y 
el término de Scala, en toda esta mesa de conceptos que estamos construyendo. Pero nuevamente hablemos de la realidad de las empresas. 
¿Qué es lo que va a pasar? el problema que tienen los clusters de big data es que son muy costosos desde un punto de vista monetario. 
Digamos que para que la empresa funcione, no sabemos nada de Sizing o de Tunning computacional, así que digamos que simplemente después 
de hacer un Sizing, es decir, cuál es el tamaño del clúster que necesitamos, decimos: “ … oye mira para que la empresa funcione y sus 
procesos corran en un núcleo de big data el cluster tiene que tener 10 servidores de 100 GB de RAM cada uno … “. De acuerdo son 
servidores empresariales linux que van a estar orientados a estar encendidos 24/7, todo el día funcionando, ejecutando procesos en la 
mañana, en la madrugada y ¿cuánto vale cada uno de estos servidores? dependiendo de las características del hardware pues 
aproximadamente 30.000 USD. ¿Cuántos necesitamos? 10, para construir el cluster. Entonces el presupuesto inicial es de 300.000 USD. 
Adicionalmente a esto, tenemos que contratar un equipo que se encargue de gestionar la infraestructura, hay que darle mantenimiento 
para revisar que el sistema operativo funcione y eso significa tener todo un equipo que sea el encargado de administrar la
infraestructura, 3 o 4 personas o los que sea necesario. ¿Anualmente cuánto nos cuesta el equipo? 100.000 USD y el mantenimiento de los 
equipos nos cuestan otros 100.000 USD, por lo tanto, necesitamos una inversión inicial de 500.000 USD para empezar a hacer el proyecto 
de Big data y cada año una inversión de 200.000 USD para pagarle al equipo que administra el clúster y el mantenimiento del clúster, 
entonces, gerencia recibe esto y por ejemplo para Microsoft o para Amazon o para Google, puedes dicirles que en total son 500.000 USD y 
para ellos no es nada y realmente te aprueban el proyecto de big data. Luego, pero salgamos de las top tecnológicas, vamos a algo un 
poco más pequeño, un banco que tenga sucursales en varias partes del mundo. Le dices: “ … el proyecto de big data nos va a salir 
500.000 USD este año y 200.000 USD el resto de del año … “ entonces el banco te puede decir: “ … bueno está un poquito caro, pero es lo 
que hacen los grandes así que lo apruebo porque yo como banco tengo la espalda financiera … “ Ahora vamos a algo más pequeño que un
banco, una cadena de supermercados o una microfinanciera o algo pues que no tenga una espalda financiera tan grande como la podría 
llegar a tener un banco, le damos estos números y los más probable que te digan: “ … está bonito el concepto, pero es demasiado 
dinero … “ y ahí murió el proyecto de Big data. Eso pasaba mucho entre los años 2006 y 2010 cuando esto salió y ahí es donde varias 
empresas vieron una oportunidad de negocio, Microsoft dijo: “ … yo tengo una granja de servidores en todo el mundo con muchísimos 
servidores que podría alquilar a bajos precios, por ejemplo, podría alquilar un server de 100 GB de RAM a 1 USD la hora …”, entonces, 
¿cuánto necesitaríamos para construir nuestro clúster de big data? son 10 servidores a 1 USD la hora, la hora de ese clúster nos 
saldría 10 USD, quizá el proceso que hemos implementado se toma 2 horas en ejecutarse, entonces, pagaríamos tan solo 20 USD, 
alquilaríamos los servidores por 2 horas, mandamos ejecutar el proceso, el proceso termina, tenemos la resultante y liberamos los 
recursos que hemos alquilado y pagaríamos tan solo 20 USD, saldría baratísimo. Ahora sí la gerencia nos aprobaría el proyecto. 
Microsoft vió esa oportunidad, lo mismo Amazon tenía su granja de servidores y lo mismo Google. Así que crearon un modelo de negocio de 
alquiler de infraestructura sobre internet o como clásicamente le llamaban en Estados Unidos “la nube”, vas a alquilar estos servidores 
que viven en internet, que viven en la nube. Así que es un modelo de negocio Cloud. Microsoft creó su servicio conocido como AZURE, 
Amazon creó su servicio conocido como AWS y Google creo su servicio conocido como GCP. Todos hacen lo mismo, te permiten alquilar 
cierta potencia computacional a precios bajos, los usas solo por el tiempo que tengas que procesar y luego libera los recursos y en la 
factura pues solo pagas por el uso que hayas hecho. 


Patrones de diseños MULTI-CLOUD
-------------------------------

Cuando nosotros desarrollemos con Spark y Scala al código le da exactamente igual si se va a ejecutar en los servidores de Azure, de 
AWS o de GCP, es solo infraestructura. Entonces el código tiene que ser agnóstico a la infraestructura o en otras palabras debe ser 
MULTI-CLOUD, por lo tanto, todos los patrones de diseño que se aprenden al día de hoy son multi-cloud, es decir, que el código debe de 
funcionar igual de bien en Azure, en AWS, en GCP o en cualquier otra nube que la empresa utilice. Todo lo que vamos a ver en 
codificación va a ser múlti-cloud, es decir, si el día de hoy te toca un cliente que usa Azure, pues, desde el día uno ya vas a saber 
cómo ejecutar todo esto sobre Azure o sobre AWS o sobre el GCP. Esto no significa que entonces el código siempre va a ser multi-cloud, 
ya que, Azure, por ejemplo, tiene sus propias tecnologías propietarias que te pueden facilitar el desarrollo de tu codificación, igual 
AWS e igual GCP. En una primera instancia para aprender de programación en entornos de big data sí es correcto que los patrones de 
diseño sean multi-cloud ese es el objetivo de este curso. Pero una vez eso, pues ¿cuál sería el siguiente paso? dependiendo de la 
empresa hay que saber cómo adaptarnos a cada tecnología de big data propietaria de cada ecosistema, una cosa será en Azure, otra en AWS, 
otra en GCP. Esto hará que, pues, el desarrollo se acelere aún más, además también hay que saber integrarlos a las distintas nubes, 
porque ellos tienen sus tecnologías propietarias. Así que ya después de esto, lo siguiente es particularizarlo, pues, a la nube en 
específico en donde se vaya a trabajar. 

También hay que entender algo más, otra tendencia que tienen las empresas es la siguiente, generalmente lo que queremos es alquilar la 
infraestructura del procesamiento, no la de almacenamiento, así que clásicamente se trabajan con 2 infraestructuras: 

-	Una permanente 
-	Una volátil 

¿Esto que significa? vamos a ponerlo en un ejemplo más concreto, aquí tenemos un sistema de archivos distribuidos por detrás habrá 20, 
30 o 50 servidores con disco duro, pero nos da exactamente igual. Supongamos que tenemos 1000 TB de almacenamiento. Ahora esta es la 
primera infraestructura que necesitamos, el sistema de archivos distribuidos. Clásicamente es HDFS, pero cada nube tiene su propia 
tecnología propietaria, por ejemplo, AWS tiene al S3, Azure tiene a la WASBS y Google tiene al GS, Databricks tiene al DBFS, todos 
hacen lo mismo, es simplemente el sistema de archivos distribuidos, de hecho, todas se basan en HDFS, solo que son tecnologías 
propietarias ya de cada nube. Nos da exactamente igual cuál sea ese sistema de archivos, con todas vamos a hacer lo mismo. Negocio nos 
da un archivo de datos a procesar, entonces, sobre el sistema de archivos vamos a crear algún directorio y vamos a subir el archivo de 
datos dentro de ese directorio, entonces, por ejemplo, digamos que negocio nos deja los archivos: persona, empresa y transacción y pues 
en la vida real negocio tiene muchas entidades de negocio. Para poner un concepto parecido al warehouse, en un warehouse hay miles y 
miles de entidades, pues lo mismo pasa en los entornos de big data, tendremos la entidad persona, empresa, transacción, cliente, 
producto, etc., todo lo que negocio necesite para funcionar. Acá habrá un directorio persona y ahí estará el archivo o archivos que 
conforman a las personas, el directorio empresa que es la entidad de empresa, el directorio transacción y si hay 20.000 entidades, 
pues, tendríamos 20.000 de estos directorios, es permanente, es la información que tiene la empresa, no es que esto se vaya a perder, 
tiene que vivir de por vida hasta que la empresa decida borrar esos archivos de datos. Eso vive en una infraestructura, conocido como 
el clúster del sistema de archivos distribuidos, es un clúster que está enfocado en solo en almacenar datos. 

AHORA, VAMOS A PROCESAR, ESO SE HACE EN UNA SEGUNDA INFRAESTRUCTURA, CUANDO HAGAMOS PROYECTOS DE BIG DATA SIEMPRE COMO MÍNIMO DEBE 
HABER 2 INFRAESTRUCTURAS SEPARADAS, LA PRIMERA DE ELLAS ES PERMANENTE EL SISTEMA DE ARCHIVOS DISTRIBUIDOS. Nunca se va a borrar este 
clúster, debe de existir encendido siempre ¿por qué? porque ahí está la información que negocio tiene, la data que vamos a procesar. LA 
INFRAESTRUCTURA DOS, POR OTRO LADO, ES VOLÁTIL, QUE ES LA INFRAESTRUCTURA DE PROCESAMIENTO. Por ejemplo, estamos en la nube, el día de 
hoy tenemos que procesar 300 GB de datos de estas 3 entidades (persona, empresa, transaccion) de las 20.000 entidades que tenga la 
empresa, solo nos interesa procesar estas 3 y son 300 GB de datos. De acuerdo vamos a crear un clúster en la nube de 3 servidores y 
tendremos 300 GB de RAM y cierta cantidad de nucleos de CPU, volcamos la información desde el disco duro en la RAM, hacemos lo que 
tenemos que hacer y tenemos la resultante y esa resultante la guardamos en el sistema de archivos distribuidos y listo, destruimos el 
clúster y solamente pagamos por el tiempo que hayamos utilizado este Cluster. Por eso es que la infraestructura de procesamiento es 
volátil, quizá el día siguiente vengan 500 GB de datos, ese día necesitaremos un cluster más grande y pues probablemente ese día 
paguemos más dinero. 

ASÍ QUE HAY QUE TENERLOS SEPARADOS SIEMPRE, INFRAESTRUCTURA DE ALMACENAMIENTO E INFRAESTRUCTURA DE PROCESAMIENTO. LA DE ALMACENAMIENTO 
ES PERMANENTE, AHÍ ESTÁN LOS DATOS INPUT Y LOS DATOS OUTPUT. EN CAMBIO, EN LA INFRAESTRUCTURA DE PROCESAMIENTO ES VOLÁTIL, SE LEE LO 
QUE SE DEBA DE PROCESAR, SE PROCESA Y LA RESULTANTE SE ESCRIBE EN EL SISTEMA DE ALMACENAMIENTO Y LUEGO DESTRUIMOS ESA INFRAESTRUCTURA Y 
PAGAMOS POR EL TIEMPO QUE HAYAMOS USADO ESA INFRAESTRUCTURA, POR ESO ES VOLÁTIL. 


Spark nos permite el procesamiento en Multi-Cloud
-------------------------------------------------

Esta es la configuración mínima que hay que tener, siempre separarlo en 2 infraestructuras. Ahora que hemos entendido esto vamos a ir 
un siguiente nivel. En la vida real es probable que en las empresas, en algún momento tengan diferentes cuentas en la nube, a esto se 
le conoce como ser multi-cloud, así que podríamos encontrarnos con el siguiente escenario, por aquí tenemos un sistema on-premise, 
on-premise significa que la propia empresa compró los servidores en sus oficinas y los instaló, entonces, ahí tenemos ese sistema 
gestionado por HDFS, con algunas entidades que la empresa cargo ahí. Y también tenemos la cuenta en nube con Amazon S3 y ahí también 
hay algunas entidades que la empresa ha subido a Amazon y también por ahí tenemos a Azure, con su sistema de WASBS, que al final es lo 
mismo. Y por ahí también tenemos a Google con su sistema GS de almacenamiento y ahí tenemos también las entidades, podría darse ese 
caso. Digamos que estamos trabajando en un cliente que tiene Azure, entonces sobre Azure levantamos un clúster de big data de 10 o 
5 servidores o los que necesitemos, vamos a procesar, con Spark tendremos que conectarnos a HDFS y leer 2 entidades y volcarlas a la 
memoria RAM del clúster y también necesitamos 2 entidades de Amazon y las volcamos en la memoria RAM del clúster y una de Azure y la 
volcamos en la memoria RAM del clúster y 3 de Google y las volcamos en la memoria RAM del cluster. En pocas palabras, hemos leído lo 
que queremos procesar desde diferentes sistemas de archivos, hacemos lo que tenemos que hacer y la resultante la tenemos también una 
variable en memoria RAM. El último paso es y ¿dónde queremos escribir el proceso? en Azure, entonces nos comunicamos con el sistema de 
archivos de Azure y mandamos a escribirlo ahí. Vean entonces que Spark ya nos habilita también el procesamiento en multi-cloud, Spark 
permite leer desde diferentes sistemas de archivos, no solo desde uno único. Van a ver que esto en código también se traduce en entender 
el protocolo del sistema de archivo, que el código es simple, básicamente el nombre del sistema de archivo seguido de : y luego la ruta 
sobre el sistema de archivos, pero ya tenemos una herramienta que nació también con esa filosofía, leemos desde diferentes nubes, 
procesamos en el clúster de la nube que queramos, tenemos la resultante y la guardamos en la nube que queramos, podría ser la propia 
nube de Azure o cualquiera otra de las nubes o dentro de nuestro repositorio local. Spark nos va a habilitar todo esto, justamente por 
eso fue el reemplazo de mapReduce, porque, no nació con todo eso, nació antes de las tecnologías Google u otras cosas que ahora 
clásicamente se estilan realizar. Por eso es que usamos Spark. 


Elasticidad
-----------

Ahora ya entendemos otro concepto, el concepto de big data nos permitió a manera de un resumen muy grande ser escalables, ya hace un
momento dimos un resumen de qué significa esto así que simplemente diré gracias al big data somos escalables y una forma de implementar 
esto es con Spark. Ahora hemos agregado el otro concepto de la nube, ya sabemos cuáles son las ventajas de la nube, quizá el día de hoy 
necesitemos solo 5 servidores y mañana 3 y mañana 2 y mañana 7, en función de cuánta data nos den. Entonces tenemos un clúster que puede 
aumentar o disminuir el número de servidores, los usamos por el tiempo que tengamos que usarlos y luego destruimos esos servidores. A 
este concepto se le conoce como ELASTICIDAD. La nube nos permite ser elásticos, es decir, permite aumentar o disminuir la potencia de 
infraestructura en función de la necesidad de la volumetría, el código hoy necesita de 5 servidores, alquilemos esos 5 servidores o 
quizás 10 y quizás mañana 20 y luego 2, entonces aumenta o disminuye, por eso es ELÁSTICO. Todo esto lo implementa Spark, así que POR 
ESO DECIMOS QUE ES SPARK NOS PERMITE TENER CÓDIGO ESCALABLE Y CÓDIGO ELÁSTICO y es de esta manera en que Spark se ha convertido en la 
herramienta de procesamiento para entornos de big data al día de hoy, por eso se utiliza mucho. Ahora eso no significa que Spark sea el 
único motor Spark, es el motor de uso general, tienes un proyecto de big data sí o sí tiene que ir a Spark. De hecho todas las nubes la 
traen, es la herramienta por defecto para programación sobre entornos de Big data, pero es de uso general, cualquier cosa que tú quieras 
hacer de big data, ni lo pienses, Spark. Pero como todo en la vida hay proyectos particulares, por ejemplo, existe el motor de 
Tensorflow que es un motor de distribución de carga de trabajo, pero orientado a procesos solo de Deep learning, ni siquiera procesos 
analíticos en general, solo los de Deep learning. Así que podríamos complementar Spark también con otros motores, pero ya son motores 
de nicho, de algo muy particular, por supuesto, estos motores ya estarían fuera del alcance de este curso, nosotros solamente nos vamos 
a enfocar en la parte de procesamiento con Spark. Con todo esto ya entendemos los conceptos más básicos de todos.