{"cells":[{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Creación de un Dataframe con una lista y .toDF"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ddd89a36-8d54-4692-9e35-46a7b92323a1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Ejemplo 1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eee1b8d2-c19c-43b0-bb77-a045a6a646e1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["empleados = [(1, 'Scott', 'Tiger', 1000.0, 'United States', '+1 123 456 7890', '123 45 6789'),\n             (2, 'Henry', 'Ford', 1250.0, 'India', '+91 234 567 8901', '456 78 9123'),\n             (3, 'Nick', 'Menza', 750.0, 'United Kingdom', '+44 111 111 1111', '222 33 4444'),\n             (4, 'Bill', 'Gates', 1500.0, 'Asutralia', '+61 987 654 3210', '789 12 6118')\n            ]\n\ndf = spark.createDataFrame(empleados).toDF('id','nombre', 'apellido', 'salario', 'nacionalidad', 'telefono', 'ssn')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"eb342d6f-f40d-4791-9ec0-6e18874f8446","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b7b2b0d4-905b-4db2-9d04-40f5e0826d3e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: long (nullable = true)\n |-- nombre: string (nullable = true)\n |-- apellido: string (nullable = true)\n |-- salario: double (nullable = true)\n |-- nacionalidad: string (nullable = true)\n |-- telefono: string (nullable = true)\n |-- ssn: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4fdc1e61-0f79-450e-a05a-3786f340bc1d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+--------+-------+--------------+----------------+-----------+\n| id|nombre|apellido|salario|  nacionalidad|        telefono|        ssn|\n+---+------+--------+-------+--------------+----------------+-----------+\n|  1| Scott|   Tiger| 1000.0| United States| +1 123 456 7890|123 45 6789|\n|  2| Henry|    Ford| 1250.0|         India|+91 234 567 8901|456 78 9123|\n|  3|  Nick|   Menza|  750.0|United Kingdom|+44 111 111 1111|222 33 4444|\n|  4|  Bill|   Gates| 1500.0|     Asutralia|+61 987 654 3210|789 12 6118|\n+---+------+--------+-------+--------------+----------------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Ejemplo 2"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4eb7de3-d5eb-4655-8309-18b4f2bb92b0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.read.format(\"csv\").\\\n     option(\"inferSchema\",True).\\\n     option(\"header\",True).\\\n     option(\"sep\",\",\").\\\n     load('/FileStore/2021-03-21/races.csv')\n\ndf.show(n=3, truncate=False, vertical=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e131cb10-3623-4fc4-bc19-aff2afc62bc1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+----+-----+---------+---------------------+----------+--------+-------------------------------------------------------+\n|raceId|year|round|circuitId|name                 |date      |time    |url                                                    |\n+------+----+-----+---------+---------------------+----------+--------+-------------------------------------------------------+\n|1     |2009|1    |1        |Australian Grand Prix|2009-03-29|06:00:00|http://en.wikipedia.org/wiki/2009_Australian_Grand_Prix|\n|2     |2009|2    |2        |Malaysian Grand Prix |2009-04-05|09:00:00|http://en.wikipedia.org/wiki/2009_Malaysian_Grand_Prix |\n|3     |2009|3    |17       |Chinese Grand Prix   |2009-04-19|07:00:00|http://en.wikipedia.org/wiki/2009_Chinese_Grand_Prix   |\n+------+----+-----+---------+---------------------+----------+--------+-------------------------------------------------------+\nonly showing top 3 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.\\\n     option(\"inferSchema\",True).\\\n     option(\"header\",True).\\\n     option(\"sep\",\",\").\\\n     csv('/FileStore/2021-03-21/races.csv').\\\n     toDF('carreraId','ano','ronda','circuitoId','nombre','fecha','hora','url')\n\ndf.show(n=3, truncate=False, vertical=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"34423f95-f5d5-4c47-8d87-6c44f12c344f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----+-----+----------+---------------------+----------+--------+-------------------------------------------------------+\n|carreraId|ano |ronda|circuitoId|nombre               |fecha     |hora    |url                                                    |\n+---------+----+-----+----------+---------------------+----------+--------+-------------------------------------------------------+\n|1        |2009|1    |1         |Australian Grand Prix|2009-03-29|06:00:00|http://en.wikipedia.org/wiki/2009_Australian_Grand_Prix|\n|2        |2009|2    |2         |Malaysian Grand Prix |2009-04-05|09:00:00|http://en.wikipedia.org/wiki/2009_Malaysian_Grand_Prix |\n|3        |2009|3    |17        |Chinese Grand Prix   |2009-04-19|07:00:00|http://en.wikipedia.org/wiki/2009_Chinese_Grand_Prix   |\n+---------+----+-----+----------+---------------------+----------+--------+-------------------------------------------------------+\nonly showing top 3 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Ejemplo 3 (Incorrecto)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d605ca2c-d250-4f3a-baf5-44133d1349b2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["columnas = ['carreraId','ano','ronda','circuitoId','nombre','fecha','hora','url']\n\ndf = spark.read.\\\n     option(\"inferSchema\",True).\\\n     option(\"header\",True).\\\n     option(\"sep\",\",\").\\\n     csv('/FileStore/2021-03-21/races.csv').\\\n     toDF(columnas)\n\ndf.show(n=3, truncate=False, vertical=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"58d99a5c-d8cd-4caf-849d-4b2661aca1f0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-619386710229849>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m columnas \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcarreraId\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mano\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mronda\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcircuitoId\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnombre\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfecha\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhora\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39m\\\n\u001B[1;32m      4\u001B[0m      option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39m\\\n\u001B[1;32m      5\u001B[0m      option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39m\\\n\u001B[1;32m      6\u001B[0m      option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msep\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39m\\\n\u001B[1;32m      7\u001B[0m      csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/2021-03-21/races.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39m\\\n\u001B[1;32m      8\u001B[0m      toDF(columnas)\n\u001B[1;32m     10\u001B[0m df\u001B[38;5;241m.\u001B[39mshow(n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, vertical\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4943\u001B[0m, in \u001B[0;36mDataFrame.toDF\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   4912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   4913\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` that with new specified column names\u001B[39;00m\n\u001B[1;32m   4914\u001B[0m \n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    .. versionchanged:: 3.4.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4941\u001B[0m \u001B[38;5;124;03m    +---+-----+\u001B[39;00m\n\u001B[1;32m   4942\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4943\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoDF\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jseq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4944\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: The number of columns doesn't match.\nOld column names (8): raceId, year, round, circuitId, name, date, time, url\nNew column names (1): [carreraId, ano, ronda, circuitoId, nombre, fecha, hora, url]","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: The number of columns doesn't match.\nOld column names (8): raceId, year, round, circuitId, name, date, time, url\nNew column names (1): [carreraId, ano, ronda, circuitoId, nombre, fecha, hora, url]","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n","File \u001B[0;32m<command-619386710229849>:3\u001B[0m\n","\u001B[1;32m      1\u001B[0m columnas \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcarreraId\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mano\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mronda\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcircuitoId\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnombre\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfecha\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhora\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m'\u001B[39m]\n","\u001B[0;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39m\\\n","\u001B[1;32m      4\u001B[0m      option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39m\\\n","\u001B[1;32m      5\u001B[0m      option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39m\\\n","\u001B[1;32m      6\u001B[0m      option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msep\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39m\\\n","\u001B[1;32m      7\u001B[0m      csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/2021-03-21/races.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39m\\\n","\u001B[1;32m      8\u001B[0m      toDF(columnas)\n","\u001B[1;32m     10\u001B[0m df\u001B[38;5;241m.\u001B[39mshow(n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, vertical\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4943\u001B[0m, in \u001B[0;36mDataFrame.toDF\u001B[0;34m(self, *cols)\u001B[0m\n","\u001B[1;32m   4912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n","\u001B[1;32m   4913\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` that with new specified column names\u001B[39;00m\n","\u001B[1;32m   4914\u001B[0m \n","\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    .. versionchanged:: 3.4.0\u001B[39;00m\n","\u001B[0;32m   (...)\u001B[0m\n","\u001B[1;32m   4941\u001B[0m \u001B[38;5;124;03m    +---+-----+\u001B[39;00m\n","\u001B[1;32m   4942\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n","\u001B[0;32m-> 4943\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoDF\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jseq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   4944\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n","\n","File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n","\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n","\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n","\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n","\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n","\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n","\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n","\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n","\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n","\n","\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: The number of columns doesn't match.\n","Old column names (8): raceId, year, round, circuitId, name, date, time, url\n","New column names (1): [carreraId, ano, ronda, circuitoId, nombre, fecha, hora, url]"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Ejemplo 4"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b22ed181-9eb9-4722-9e90-71a925f806ea","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["columnas = ['carreraId','ano','ronda','circuitoId','nombre','fecha','hora','url']\nprint(*columnas)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c6bb52d2-34b0-4379-8074-a5f87245e05e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["carreraId ano ronda circuitoId nombre fecha hora url\n"]}],"execution_count":0},{"cell_type":"code","source":["columnas = ['carreraId','ano','ronda','circuitoId','nombre','fecha','hora','url']\n\ndf = spark.read.\\\n     option(\"inferSchema\",True).\\\n     option(\"header\",True).\\\n     option(\"sep\",\",\").\\\n     csv('/FileStore/2021-03-21/races.csv').\\\n     toDF(*columnas)\n\ndf.show(n=3, truncate=False, vertical=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7b92e2fa-4f8f-4180-a031-843f00932c08","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----+-----+----------+---------------------+----------+--------+-------------------------------------------------------+\n|carreraId|ano |ronda|circuitoId|nombre               |fecha     |hora    |url                                                    |\n+---------+----+-----+----------+---------------------+----------+--------+-------------------------------------------------------+\n|1        |2009|1    |1         |Australian Grand Prix|2009-03-29|06:00:00|http://en.wikipedia.org/wiki/2009_Australian_Grand_Prix|\n|2        |2009|2    |2         |Malaysian Grand Prix |2009-04-05|09:00:00|http://en.wikipedia.org/wiki/2009_Malaysian_Grand_Prix |\n|3        |2009|3    |17        |Chinese Grand Prix   |2009-04-19|07:00:00|http://en.wikipedia.org/wiki/2009_Chinese_Grand_Prix   |\n+---------+----+-----+----------+---------------------+----------+--------+-------------------------------------------------------+\nonly showing top 3 rows\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"14-creación_de_un_dataframe_con_una_lista_y_toDF","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
