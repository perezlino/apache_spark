{"cells":[{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Función union y unionByName"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be5ba145-acd9-4759-8495-f782d7a1075f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nempleados = [(100,'Alfonso','1999','100','H',20000),\n             (200,'Maria','2002','200','M',40000),\n             (300,'Javiera','2010','100','M',15000),\n             (400,'Tomas','2012','300','H',30000)\n             ]\n\ncolumnas = ['id','nombre','año_ingreso','depto_id','genero','salario']\n\ndf = spark.createDataFrame(empleados, schema=columnas)\n\ndf.printSchema()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b99914a7-050e-44a8-84f4-bda5a2671bec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: long (nullable = true)\n |-- nombre: string (nullable = true)\n |-- año_ingreso: string (nullable = true)\n |-- depto_id: string (nullable = true)\n |-- genero: string (nullable = true)\n |-- salario: long (nullable = true)\n\n+---+-------+-----------+--------+------+-------+\n| id| nombre|año_ingreso|depto_id|genero|salario|\n+---+-------+-----------+--------+------+-------+\n|100|Alfonso|       1999|     100|     H|  20000|\n|200|  Maria|       2002|     200|     M|  40000|\n|300|Javiera|       2010|     100|     M|  15000|\n|400|  Tomas|       2012|     300|     H|  30000|\n+---+-------+-----------+--------+------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: long (nullable = true)\n |-- nombre: string (nullable = true)\n |-- año_ingreso: string (nullable = true)\n |-- depto_id: string (nullable = true)\n |-- genero: string (nullable = true)\n |-- salario: long (nullable = true)\n\n+---+-------+-----------+--------+------+-------+\n| id| nombre|año_ingreso|depto_id|genero|salario|\n+---+-------+-----------+--------+------+-------+\n|100|Alfonso|       1999|     100|     H|  20000|\n|200|  Maria|       2002|     200|     M|  40000|\n|300|Javiera|       2010|     100|     M|  15000|\n|400|  Tomas|       2012|     300|     H|  30000|\n+---+-------+-----------+--------+------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["empleados = [(400,'Tomas','2012','300','H',30000),\n             (500,'Juan','2002','200','H',40000),\n             (600,'Camila','2010','100','M',15000),\n             (700,'Sofia','2012','300','M',30000)\n             ]\n\ncolumnas = ['id','nombre','año_ingreso','depto_id','genero','salario']\n\ndf_2 = spark.createDataFrame(empleados, schema=columnas)\n\ndf_2.printSchema()\ndf_2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51818d99-7ad7-45b5-9ac5-edbbeabcad3c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: long (nullable = true)\n |-- nombre: string (nullable = true)\n |-- año_ingreso: string (nullable = true)\n |-- depto_id: string (nullable = true)\n |-- genero: string (nullable = true)\n |-- salario: long (nullable = true)\n\n+---+------+-----------+--------+------+-------+\n| id|nombre|año_ingreso|depto_id|genero|salario|\n+---+------+-----------+--------+------+-------+\n|400| Tomas|       2012|     300|     H|  30000|\n|500|  Juan|       2002|     200|     H|  40000|\n|600|Camila|       2010|     100|     M|  15000|\n|700| Sofia|       2012|     300|     M|  30000|\n+---+------+-----------+--------+------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: long (nullable = true)\n |-- nombre: string (nullable = true)\n |-- año_ingreso: string (nullable = true)\n |-- depto_id: string (nullable = true)\n |-- genero: string (nullable = true)\n |-- salario: long (nullable = true)\n\n+---+------+-----------+--------+------+-------+\n| id|nombre|año_ingreso|depto_id|genero|salario|\n+---+------+-----------+--------+------+-------+\n|400| Tomas|       2012|     300|     H|  30000|\n|500|  Juan|       2002|     200|     H|  40000|\n|600|Camila|       2010|     100|     M|  15000|\n|700| Sofia|       2012|     300|     M|  30000|\n+---+------+-----------+--------+------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### union\n\nLa función PySpark union() se utiliza para combinar dos o más data frames que tienen la misma estructura o esquema. Esta función devuelve un error si el esquema de los data frames difiere entre sí.\n\nSe recomienda utilizar Union por sobre UnionAll, dado que este último ya está en desuso. Con los dos obtengo el mismo resultado."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0614234-7092-4f5c-a1cf-0b08b8c65f2d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.union(df_2).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6fb5bfc-068c-47be-9c9e-1b999dd11bd6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-------+-----------+--------+------+-------+\n| id| nombre|año_ingreso|depto_id|genero|salario|\n+---+-------+-----------+--------+------+-------+\n|100|Alfonso|       1999|     100|     H|  20000|\n|200|  Maria|       2002|     200|     M|  40000|\n|300|Javiera|       2010|     100|     M|  15000|\n|400|  Tomas|       2012|     300|     H|  30000|\n|400|  Tomas|       2012|     300|     H|  30000|\n|500|   Juan|       2002|     200|     H|  40000|\n|600| Camila|       2010|     100|     M|  15000|\n|700|  Sofia|       2012|     300|     M|  30000|\n+---+-------+-----------+--------+------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-------+-----------+--------+------+-------+\n| id| nombre|año_ingreso|depto_id|genero|salario|\n+---+-------+-----------+--------+------+-------+\n|100|Alfonso|       1999|     100|     H|  20000|\n|200|  Maria|       2002|     200|     M|  40000|\n|300|Javiera|       2010|     100|     M|  15000|\n|400|  Tomas|       2012|     300|     H|  30000|\n|400|  Tomas|       2012|     300|     H|  30000|\n|500|   Juan|       2002|     200|     H|  40000|\n|600| Camila|       2010|     100|     M|  15000|\n|700|  Sofia|       2012|     300|     M|  30000|\n+---+-------+-----------+--------+------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Eliminar registros duplicados\n\n**distinct( )** y **dropDuplicates( )** cumplen la misma función."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"33b06e60-24a0-45ad-a751-b8ae215b652d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.union(df_2).dropDuplicates().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8039d12e-7a07-4c8e-8743-74c5e22d34be","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-------+-----------+--------+------+-------+\n| id| nombre|año_ingreso|depto_id|genero|salario|\n+---+-------+-----------+--------+------+-------+\n|100|Alfonso|       1999|     100|     H|  20000|\n|200|  Maria|       2002|     200|     M|  40000|\n|300|Javiera|       2010|     100|     M|  15000|\n|400|  Tomas|       2012|     300|     H|  30000|\n|500|   Juan|       2002|     200|     H|  40000|\n|600| Camila|       2010|     100|     M|  15000|\n|700|  Sofia|       2012|     300|     M|  30000|\n+---+-------+-----------+--------+------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-------+-----------+--------+------+-------+\n| id| nombre|año_ingreso|depto_id|genero|salario|\n+---+-------+-----------+--------+------+-------+\n|100|Alfonso|       1999|     100|     H|  20000|\n|200|  Maria|       2002|     200|     M|  40000|\n|300|Javiera|       2010|     100|     M|  15000|\n|400|  Tomas|       2012|     300|     H|  30000|\n|500|   Juan|       2002|     200|     H|  40000|\n|600| Camila|       2010|     100|     M|  15000|\n|700|  Sofia|       2012|     300|     M|  30000|\n+---+-------+-----------+--------+------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### unionByName\n\nLa función PySpark unionByName() también se utiliza para combinar dos o más data frames, pero podría utilizarse para combinar dataframes que tienen diferentes esquemas. Esto se debe a que combina dataframes por el nombre de la columna y no por el orden de las columnas."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"17e5fb1e-2757-46a9-9ffc-e4a96c63ca66","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### Ejemplo 1\n\nEn este ejemplo, ambos data frames, data_frame1 y data_frame2 tienen el mismo esquema."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d95502c-44e3-4308-a6e3-17dd33206ea6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data_frame1 = spark.createDataFrame(\n\t[(\"Bhuwanesh\", 82.98), (\"Harshit\", 80.31)],\n\t[\"Student Name\", \"Overall Percentage\"]\n)\n\ndata_frame2 = spark.createDataFrame(\n\t[(\"Naveen\", 91.123), (\"Piyush\", 90.51)],\n\t[\"Student Name\", \"Overall Percentage\"]\n)\n\nanswer = data_frame1.unionByName(data_frame2)\n\nanswer.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4076525e-71da-41f1-b0cf-a56b23a461fc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------+------------------+\n|Student Name|Overall Percentage|\n+------------+------------------+\n|   Bhuwanesh|             82.98|\n|     Harshit|             80.31|\n|      Naveen|            91.123|\n|      Piyush|             90.51|\n+------------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------+------------------+\n|Student Name|Overall Percentage|\n+------------+------------------+\n|   Bhuwanesh|             82.98|\n|     Harshit|             80.31|\n|      Naveen|            91.123|\n|      Piyush|             90.51|\n+------------+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Ejercicio 2\n\nEn este ejemplo, data_frame1 y data_frame2 tienen un esquema diferente pero la salida es la deseada."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"14eeb6bf-86a1-4d6b-958e-c8229c30c7ee","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data_frame1 = spark.createDataFrame(\n\t[(\"Bhuwanesh\", 82.98), (\"Harshit\", 80.31)],\n\t[\"Student Name\", \"Overall Percentage\"]\n)\n\ndata_frame2 = spark.createDataFrame(\n\t[(91.123, \"Naveen\"), (90.51, \"Piyush\"), (87.67, \"Hitesh\")],\n\t[\"Overall Percentage\", \"Student Name\"]\n)\n\nanswer = data_frame1.unionByName(data_frame2)\n\nanswer.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7304f01-5f32-4064-a492-da638cd714dd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------+------------------+\n|Student Name|Overall Percentage|\n+------------+------------------+\n|   Bhuwanesh|             82.98|\n|     Harshit|             80.31|\n|      Naveen|            91.123|\n|      Piyush|             90.51|\n|      Hitesh|             87.67|\n+------------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------+------------------+\n|Student Name|Overall Percentage|\n+------------+------------------+\n|   Bhuwanesh|             82.98|\n|     Harshit|             80.31|\n|      Naveen|            91.123|\n|      Piyush|             90.51|\n|      Hitesh|             87.67|\n+------------+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Ejercicio 3\n\nConsideremos ahora dos data frames que contienen un número desigual de columnas (esquema totalmente diferente). En este caso, necesitamos pasar un argumento adicional \"allowMissingColumns = True\" a la función unionByName."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"690aee96-ecad-4b55-861c-687cb6555484","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data_frame1 = spark.createDataFrame(\n\t[(\"Bhuwanesh\", 82.98, \"Computer Science\"),\n\t(\"Harshit\", 80.31, \"Information Technology\")],\n\t[\"Student Name\", \"Overall Percentage\", \"Department\"]\n)\n\ndata_frame2 = spark.createDataFrame(\n\t[(\"Naveen\", 91.123), (\"Piyush\", 90.51)],\n\t[\"Student Name\", \"Overall Percentage\"]\n)\n\nres = data_frame1.unionByName(data_frame2, allowMissingColumns=True)\n\nres.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51eaedf8-f71e-4ace-88e7-53c066d74d78","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------+------------------+----------------------+\n|Student Name|Overall Percentage|Department            |\n+------------+------------------+----------------------+\n|Bhuwanesh   |82.98             |Computer Science      |\n|Harshit     |80.31             |Information Technology|\n|Naveen      |91.123            |null                  |\n|Piyush      |90.51             |null                  |\n+------------+------------------+----------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------+------------------+----------------------+\n|Student Name|Overall Percentage|Department            |\n+------------+------------------+----------------------+\n|Bhuwanesh   |82.98             |Computer Science      |\n|Harshit     |80.31             |Information Technology|\n|Naveen      |91.123            |null                  |\n|Piyush      |90.51             |null                  |\n+------------+------------------+----------------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"41-funcion_union_y_unionByName","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4279347458697800}},"nbformat":4,"nbformat_minor":0}
